
Computer science : the study of how computers work and how we can use them to solve problems.

Artificial Intelligence (AI) : a field of computer science that focuses on creating machines that can think, learn, and act in ways similar to humans.

Artificial Intelligence     Broadest field
Machine Learning            Subfield of AI
Deep Learning               Subfield of ML

The Major Branches of AI

Machine Learning (ML)
    Teaches computers to learn from data without being explicitly programmed.
    Predicting stock prices or spam filtering
Natural Language Processing (NLP)   
    Allows machines to understand, interpret, and respond to human language.    
    Chatbots, translation apps
Deep Learning (DL)
    A subset of ML using neural networks to learn patterns from large datasets. 
    Image recognition, speech recognition
Computer Vision 
    Enables computers to “see” and interpret visual data.   
    Face detection, object recognition
Robotics    
    Combines AI with hardware to perform tasks in the physical world.   
    Drones, robotic arms, autonomous vehicles
Expert Systems  
    AI programs that mimic decision-making of human experts.    
    Medical diagnosis systems


RNN (Recurrent Neural Network)  = model used within NLP for sequence tasks like text and speech.
LSTM (Long Short Term Memory)   = advanced version of RNN (Store more History than RNN)
Transformer model architecture  = relies solely on a self-attention mechanism
LLM (Large Language Modal)      = A very large Transformer-based NLP model trained on massive text data.

RNN LSTM and LLM all can use to understanding and generating purpose

AI -> ML -> Deep Learning (Neural Network) -> NLP -> RNN, LSTM, Tranformers 

Artificial Intelligence
   └── Machine Learning
         └── Neural Network
               └── Deep Learning
                     └── Computer Vision
                     └── NLP (Language Understanding)
                           └── Transformer
                                 └── Large Language Model (LLM) {Use case - GenAI}


---------------------------------------------------------------------------------------------------

Transformers

Transformers were introduced in the 2017 paper “Attention Is All You Need” and are now the foundation of models like GPT
Transformers are a type of neural network architecture designed to handle sequence data (like text, audio, or time series) by relying on attention mechanisms

---------------------------------------------------------------------------------------------------

LLMs

Large Data          → Trained on trillions of words from books, websites, code, and conversations.
Large Parameters    → billions or even trillion of neural weights (the “knowledge” inside the model).

Working of LLM
----------------
Input               -   User sends message (text or voice)
Tokenization        -   Split into small language units
Embedding           -   Convert tokens → vectors
Positional Encoding -   Tells where each word is in the sentence.
Transformer Layers  -   Apply self-attention to find relationships
Output Prediction   -   Predict next token (word/subword)
Detokenization      -   Convert model output → text
Response Generation -   Return final output to the user


Deterministic Modal     -   with same input give same answer (ex : path Find modal)
Non-Deterministic Modal -   with same input give different answer (ex : LLM)


Embedding
    Meaning compressed into numbers so machines can think.
    An embedding is a dense numerical vector that represents the semantic meaning of data (text, image, audio, code).
    Embeddings map high-dimensional semantic information into a dense vector space where similarity corresponds to meaning.

Positional Encoding
    Positional Encoding tells the model where each word is in the sentence.
    Since transformers lack Sequencing (present in RNN/LSTM) and process all tokens in parallel, positional encoding provides sequence order information, enabling attention mechanisms to model word relationships correctly.

Vector Database
    Enables efficient similarity search over high-dimensional embeddings using approximate nearest neighbor (ANN) algorithms.

---------------------------------------------------------------------------------------------------

Agentic AI:
    AI systems that can take actions on their own to achieve a goal
    Not just answer questions or generate text like LLMs
    Example :
        "Book the cheapest Delhi–Bangalore flight tomorrow under ₹4,500 and email me the ticket"

LLM = Brain (reasoning)
Agent = Body (taking actions)


---------------------------------------------------------------------------------------------------

LRM (Large Reasoning Model)
    A Large Reasoning Model is the next generation after LLMs.

LLM = Large Language Model → good at language
LRM = Large Reasoning Model → good at thinking, logic, and solving problems

Analogy:
-----------
LLM = Student who speaks English fluently
LRM = Student who solves problems, does math, gives proofs, then explains in English

The Reasoning Engine is what makes it an LRM.

Reasoning Engine: What’s Inside?

LRMs use specialised reasoning techniques:

Inside Reasoning Engine :
----------------------------
1.  Chain-of-Thought (CoT)   -   Step-by-step thinking.
2.  Tree-of-Thought (ToT)    -   Considers multiple possibilities before choosing the best.
3.  Graph-of-Thought         -   Handles complex relations like knowledge graphs.
4.  Planning Modules         -   Used in agents to create task plans.
5.  Structured Memory        -   Stores intermediate facts.
6.  External Tools           -   Uses calculators, search, code execution, APIs.


---------------------------------------------------------------------------------------------------

AGI (Artificial General Intelligence)

This is a future technology

AGI means an AI that can think, learn, and understand anything that a human can.
Do it across many domains.
Not just language.

Key Features of AGI
---------------------
1.  Generalization       -   Can learn one skill and apply it to another.(Humans do this naturally.)
2.  Reasoning            -   Thinks step by step to solve new problems.
3.  Autonomy             -   Works without constant instructions.
4.  Learning Abilities   -   Learns from experience continuously.
5.  Self-Improvement     -   Gets better over time by itself.

LLM → LRM (Reasoning Models) → Agentic AI → AGI

Agentic AI + LRMs are considered pre-AGI layers (pre-AGI Stage).


---------------------------------------------------------------------------------------------------

ASI (Artificial Super-Intelligence)

This is a future technology

ASI means an AI that is smarter than the best human minds in every domain.

Power if ASI:
--------------
    Simulate the entire world
    Improve itself continuously
    Plan long-term global strategies
    Understand every scientific domain
    Create new technologies every minute
    Design new drugs, machines, materials
    Cure diseases faster than human scientists
    Solve climate change with perfect optimization


---------------------------------------------------------------------------------------------------

MCP (Model Context Protocol)

MCP is an standard protocol for connecting AI models (like LLMs) to external tools.
AI Agent uses MCP to access data sources, or systems in a structured way.
It uses a client–server architecture: AI applications (hosts) talk to MCP servers via MCP clients.

AI applications <-> MCP Client <-> MCP Server <-> Resources (Data, Tools, etc )


---------------------------------------------------------------------------------------------------

MOE (Mixture Of Experts)

MOE is a neural network architecture where multiple “expert” models exist
A “Gating Network” chooses which experts to activate for each input.

Instead of one big model doing all the work…
Many small specialized models ("experts") sit inside
A "gate" selects the best experts for each token or task

DeepSeek R1, Gemini 1.5, GPT-4/5 use MoE.


---------------------------------------------------------------------------------------------------

Multimodal AI

AI systems that can understand, process, and generate information from multiple types of data (modalities) such as:
Text
Images
Audio
Video
Sensors / 3D data
Speech








Dout
--------------

Vector Database

Multimodal AI.
RAG (Retrival Augmented Genration)
Context window
Embadding Model
Prompt
Retriever
Hidden state
SELF-ATTENTION in transforms
Neural weights == Nural nodes (no. neural weights == no. Nural nodes) ?
Feed Forward Layers
Multimodal model
Converts each token ID → vector 
agent frameworks like LangChain, AutoGPT, CrewAI
hallucinations in LLM
easoning Engine in Large Reasoning Model
Vision Transformers (ViTs)
Generative Adversarial Networks (GANs)
attention in memory in Transformers

Positional Encoding
Attention mechanism
Tokenization (BPE, WordPiece)
Vector Databases (FAISS, Pinecone)


